{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Creating a new inference engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An inference engine is essentially a wrapper around the system that runs inference, either on an LLM or a TTS. The inference engine is responsible for converting the standardized data structures of Nova into the format the specific system expects, as well as converting the output back into one of the standardized datastructures of Nova. Due to the modular nature of Nova, you can easily add your own inference engine. This notebook shows you how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an LLM inference engine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to ./Nova2/app/inference_engines/inference_llm and create a new python file. The convention is to name the file \"inference_nameofyourservice.py\" but this is entirely optional. Open the file and begin to import a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .inference_base_llm import InferenceEngineBaseLLM\n",
    "from ...tool_data import *\n",
    "from ...llm_data import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class that inherits from \"InferenceEngineBaseLLM\" and call the constructor of the parent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceEngine(InferenceEngineBaseLLM):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # Run further setup code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now overwrite a few methods that will be called when the inference engine is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(self, conditioning: LLMConditioning) -> None:\n",
    "    \"\"\"\n",
    "    Here is where you set up your model based on the conditioning. If you are using a local solution,\n",
    "    this method is where you load the model into memory.\n",
    "    \"\"\"\n",
    "\n",
    "def run_inference(self, conversation: Conversation, tools: List[LLMTool] | None) -> LLMResponse:\n",
    "    \"\"\"\n",
    "    This is where you unpack the conversation object and the tool list (if present) and run model inference.\n",
    "    From the output, construct an \"LLMResponse\" object and return it.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to add the inference engine to ./Nova2/app/inference_engines/_\\_init__.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .inference_llm.inference_nameofyourservice.py import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now select your inference engine via the API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a TTS inference engine:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigate to ./Nova2/app/inference_engines/inference_tts and create a new python file. The convention is to name the file \"inference_nameofyourservice.py\" but this is entirely optional. Open the file and begin to import a few things:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .inference_base_tts import InferenceEngineBaseTTS\n",
    "from ...tts_data import TTSConditioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a class that inherits from \"InferenceEngineBaseTTS\" and call the constructor of the parent class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceEngine(InferenceEngineBaseTTS):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # Run further setup code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now overwrite a few methods that will be called when the inference engine is used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(self, model: str) -> None:\n",
    "    \"\"\"\n",
    "    Here is where you set up your model. If you are using a local solution,\n",
    "    this method is where you load the model into memory.\n",
    "    \"\"\"\n",
    "\n",
    "def run_inference(self, text: str, conditioning: TTSConditioning) -> bytes:\n",
    "    \"\"\"\n",
    "    This is where you run inference on the model based on the conditioning as well as return the audio data.\n",
    "    Note that the audio data must be in mp3 or wave format.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is to add the inference engine to ./Nova2/app/inference_engines/_\\_init__.py:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from .inference_tts.inference_nameofyourservice.py import *"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
