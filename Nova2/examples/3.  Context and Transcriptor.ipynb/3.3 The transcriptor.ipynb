{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.3 The transcriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transcriptor is a built in system that can read an audio stream from the microphone and transcribe speech into text using OpenAI's [Whisper](https://github.com/openai/whisper). It also generates speaker embeddings which are high dimensional vectors that represent the voice of a speaker which can be used to differentiate between multiple speakers. The transcriptor used in Nova is a slighly modified version of my [Voice Analysis Toolkit](https://github.com/00Julian00/Voice-Analysis-Toolkit). This notebook will show you how to use the transcriptor and how to connect it to the context system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code so python can find the scripts. This is not required when importing Nova from outside the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = Path().absolute().parent.parent\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.append(str(module_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nova import *\n",
    "\n",
    "nova = Nova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the transcriptor mirrors how the LLM and TTS system is set up, except that you only need a conditioning object and no inference engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditioning = TranscriptorConditioning(\n",
    "    microphone_index=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you pass the correct microphone index of the device you intend to use. You can find a list of all microphones and their indices by using the \"sounddevice\" library (which you should already have installed if you installed the requirements):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "\n",
    "sd.query_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, configure the transcriptor and apply the configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova.configure_transcriptor(conditioning=conditioning)\n",
    "\n",
    "nova.apply_config_transcriptor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now want to start the transcriptor. This will give us a \"ContextGenerator\" object which is esentially a wrapper for all systems that continously yield context data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_generator = nova.start_transcriptor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically add the yielded context data to the context, we need to bind the context generator to the context system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova.bind_context_source(source=context_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all of the data produced by the transcriptor will automatically be added to the context.  \n",
    "Additionally, any voices the transcriptor encounters will be stored in a database as a voice embedding. They will initially be represented as \"UnknownVoiceX\" in the context, but the LLM can rename the voice if it learns your name. Note that this is only possible if default tools are loaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional parameters of \"TranscriptorConditoning\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model: Which whisper model to use. You can find all available models [here](https://github.com/openai/whisper?tab=readme-ov-file#available-models-and-languages)\n",
    "- device: Choose between \"cuda\" and \"cpu\".\n",
    "- voice_boost: How much the speech's volume should be boosted compared to other sounds in the audio data whisper receives. Can be usefull in noisy environments.\n",
    "- language: Must be a valid language code or \"None\". Force whisper to interpret the speech in a certain language. Can improve results if you are only talking in one language. Set to \"None\" to let whisper automatically determine the spoken language.\n",
    "- vad_threshold: The threshold the voice-activity-detection system needs to surpass in its evaluation wether an audio chunk contains speech."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
