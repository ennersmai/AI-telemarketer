{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.2 Tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tools are scripts that can extend the functionality of an LLM. It allows the LLM to run scripts and parse parameters at the appropriate time. This notebook shows you how to load tools and execute them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code so python can find the scripts. This is not required when importing Nova from outside the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = Path().absolute().parent.parent\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.append(str(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start to set up our LLM like in [1.1](1.1%20running%20inference.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nova import *\n",
    "\n",
    "nova = Nova()\n",
    "\n",
    "inference_engine = InferenceEngineLlamaCPP()\n",
    "conditioning = LLMConditioning(\n",
    "    model=\"bartowski/Qwen2.5-7B-Instruct-1M-GGUF\",\n",
    "    file=\"*Q8_0.gguf\"\n",
    ")\n",
    "\n",
    "nova.configure_llm(inference_engine=inference_engine, conditioning=conditioning)\n",
    "nova.apply_config_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the LLM can call tools, we need to load them first.                                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = nova.load_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"tools\" now contains all our loaded tools. Note that if a tool failed to load it will not be in the list.  \n",
    "All we need to do now is to parse our list of tools to the LLM when running inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation()\n",
    "\n",
    "message = \"Your message here\"\n",
    "user_message = Message(author=\"user\", content=message)\n",
    "conversation.add_message(user_message)\n",
    "\n",
    "llm_response = nova.run_llm(conversation=conversation, tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left is to execute the tool calls. This will run the scripts associated with the tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova.execute_tool_calls(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"load_tools()\" loads all tools found in the \"tools\" folder, but you also have more fine-grained control over which tools are loaded.  \n",
    "Parameters:  \n",
    "- load_internal_tools: Wether the built-in tools should be loaded. It is recommended set this to \"True\", as otherwise the LLM loses access to some core functionality.  \n",
    "- include: A list of tools you want to load. Acts as a whitelist. Is incompatible with \"exclude\".  \n",
    "- exclude: A list of tools you don't want to load. Acts as a blacklist. Is incompatible with \"include\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
