{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.3 Long-term memories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nova has a built in memory system that uses retrieval-augmented generation to feed additional information to the model it has memorized. This notebook shows you how you can give the model access to these memories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code so python can find the scripts. This is not required when importing Nova from outside the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = Path().absolute().parent.parent\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.append(str(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step is to set up the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nova import *\n",
    "\n",
    "nova = Nova()\n",
    "\n",
    "inference_engine = InferenceEngineLlamaCPP()\n",
    "conditioning = LLMConditioning(\n",
    "    model=\"bartowski/Qwen2.5-7B-Instruct-1M-GGUF\",\n",
    "    file=\"*Q8_0.gguf\"\n",
    ")\n",
    "\n",
    "nova.configure_llm(inference_engine=inference_engine, conditioning=conditioning)\n",
    "nova.apply_config_llm()\n",
    "\n",
    "nova.load_tools()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To give the LLM access to memories, we will need to create a MemoryConfig object. Similar to the LLMConditioning, this will store all parameters required to access memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_config = MemoryConfig(\n",
    "    retrieve_memories=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now when running inference on the LLM, just parse the memory_config object to give the LLM access to memories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = Conversation()\n",
    "\n",
    "message = \"Your message here\"\n",
    "user_message = Message(author=\"user\", content=message)\n",
    "conversation.add_message(user_message)\n",
    "\n",
    "llm_response = nova.run_llm(conversation=conversation, memory_config=memory_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Storing memories is enabled by default. It is one of the default tools the LLM has access to. Note that the LLM can not store new memories if tools were not loaded or \"load_default_tools\" is False. More on tools [here](1.2%20tool%20use.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How memory retrieval works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memories use a system called \"Retrieval-augmented-generation\". If the LLM decides to store a new memory it will first be converted into a text embedding which is just a high-dimensional vector. You can think of a text embedding as representing the meaning of a text. This embedding is then stored in a database. When the user now prompts the LLM, their message is split by sentence and each sentence is individually converted to text embeddings. Using a similarity search, the system tries to find entries that are close in meaning to the given text. If a result is found that surpasses the threshold, it and its surrounding entries are parsed to the LLM. This is commonly known as \"neighbourhood retrieval\" and it serves to parse more potentially important context to the LLM. This is important to know so you understand the parameters of the MemoryConfig object:  \n",
    "- retrieve_memories: Wether to even retrieve any memories.\n",
    "- num_results: Only use up to x results above the threshold.\n",
    "- search_area: How many entries before and after the result should also be parsed to the LLM.\n",
    "- cosine_threshold: The similarity score an entry must surpass to be considered a result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
