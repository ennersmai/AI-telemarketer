{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.1 Running inference on an LLM (Large-Language-Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM is the core of the whole Nova system. It understands the users request and can not only answer them but also act on them using tools. This notebook will show you how to interact with the LLM system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code so python can find the scripts. This is not required when importing Nova from outside the root folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "module_path = Path().absolute().parent.parent\n",
    "if str(module_path) not in sys.path:\n",
    "    sys.path.append(str(module_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Nova class and create an instance. This is the API for Nova and everything you can do with Nova you do through this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nova import *\n",
    "\n",
    "nova = Nova()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run inference on an LLM, we need to set up a few things.  \n",
    "You will need to choose an inference engine. This is essentially what service/system you want to use to run the LLM. By default Nova comes with 2 inference engine. One using [LlamaCPP](https://github.com/ggml-org/llama.cpp) and one using [Groq](https://groq.com/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLamaCPP:\n",
    "inference_engine = InferenceEngineLlamaCPP()\n",
    "\n",
    "# Groq:\n",
    "inference_engine = InferenceEngineGroq()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will need to create an LLMConditioning. This is esentially an object that contains all parameters required to run the LLM. Note that the exact parameters can vary from engine to engine. Below are examples for both LlamaCPP and Groq."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLamaCPP conditioning:\n",
    "conditioning = LLMConditioning(\n",
    "    model=\"bartowski/Qwen2.5-7B-Instruct-1M-GGUF\",\n",
    "    file=\"*Q8_0.gguf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:  \n",
    "- model: This must be a valid huggingface repo ID. Note that the model must be in GGUF format.  \n",
    "- file: The name of the file to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Groq conditioning:\n",
    "conditioning = LLMConditioning(\n",
    "    model=\"llama-3.2-90b-vision-preview\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:  \n",
    "- model: Which model to use. Make sure it is one of the model hosted on [Groq](https://groq.com/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using Groq as your inference engine, you will need to set your API key. The key will be encrypted and stored in a database. Next time Nova needs the API key, it will just retrieve it in the background so you do not have to parse it everytime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova.edit_secret(Secrets.GROQ_API, \"YOUR-API-KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are using an engine that pulls the model from huggingface it is also a good idea to log in so you gain access to gated repos. Just like API keys, your huggingface token will be encrypted and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova.huggingface_login(overwrite=True, token=\"YOUR-HF-TOKEN\")\n",
    "\n",
    "# Next time you want to log into huggingface run:\n",
    "nova.huggingface_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to configure the LLM system by parsing the inference engine and the conditioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nova.configure_llm(inference_engine=inference_engine, conditioning=conditioning)\n",
    "\n",
    "# You need to apply your new configuration.\n",
    "# Only after applying the configuration will the model be loaded into memory.\n",
    "nova.apply_config_llm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM is now ready to be used. The LLM system takes in a \"Conversation object\" containing a list of messages.  \n",
    "Here is a simple example showing you how to set up a chat with the LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a new conversation\n",
    "conversation = Conversation()\n",
    "\n",
    "# 2. Create a message object\n",
    "message = \"What are the benefits of open source AI?\"\n",
    "user_message = Message(author=\"user\", content=message)\n",
    "\n",
    "# 3. Add the message to the conversation\n",
    "conversation.add_message(user_message)\n",
    "\n",
    "# 4. Run the LLM.\n",
    "llm_response = nova.run_llm(conversation=conversation)\n",
    "\n",
    "# 5. Print the result of the LLM\n",
    "llm_response_text = llm_response.message\n",
    "print(f\"Assistant: {llm_response_text}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
