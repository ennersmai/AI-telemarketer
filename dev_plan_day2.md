# Development Plan - Day 2: Refactoring for Native Nova2 Integration

Based on the analysis that Nova2 has a native `VoiceAnalysis` component designed for integrated STT/VAD, which the current telemarketer code bypasses, this plan outlines the refactoring steps to align the application with the Nova2 architecture, implement local STT/TTS, and prepare for future enhancements.

**Goal:** Refactor the FastAPI application (`telemarketer_server.py` and related modules) to correctly utilize the `nova2` framework's native architecture, implement local STT (Whisper Medium) and configurable TTS (Zonos primary, ElevenLabs option), ensure robust Twilio WebSocket integration, prepare for dashboard integration, and structure LLM prompt integration.

---

**Task 1: Refactor STT Integration (Use Native `VoiceAnalysis`)**

- [ ] **Subtask 1.1: Analyze `VoiceAnalysis` & VAD Configuration**
    - Review `Nova2/app/transcriptor.py` (likely contains `VoiceAnalysis`) and `Nova2/examples/3.  Context and Transcriptor.ipynb/3.3 The transcriptor.ipynb`.
    - Identify `TranscriptorConditioning` parameters related to VAD (e.g., `vad_threshold` for Silero VAD sensitivity) and Whisper model selection (`model`, `device`).
    - Determine default VAD behavior and necessary tuning parameters for call center audio.

- [ ] **Subtask 1.2: Adapt `VoiceAnalysis` for WebSocket Audio Input**
    - **Strategy:** Instead of modifying `VoiceAnalysis` directly, create a custom audio input stream/generator within `telemarketer_server.py`.
    - In the `/twilio/stream` WebSocket handler, receive audio chunks (`media` messages).
    - Feed these chunks into an `asyncio.Queue` or similar buffer accessible by the `VoiceAnalysis` input process.
    - Modify/Wrap the audio *reading* part accessed by `VoiceAnalysis` (which likely uses `sounddevice` or similar internally) to read from this queue instead of a physical device. This might involve creating a custom "device" or input loop that `VoiceAnalysis` can use.
    - Ensure proper format conversion (e.g., Twilio's mulaw to PCM if needed before VAD/Whisper). Note: The example `stt_integration.py` assumed Twilio PCM; verify this.

- [ ] **Subtask 1.3: Configure `VoiceAnalysis` for Whisper Medium**
    - In the server's startup/configuration phase, create `TranscriptorConditioning`:
        - Set `model` to the identifier for Whisper Medium (e.g., "medium" or "medium.en"). Confirm the exact name expected by Nova2's Whisper implementation (likely Faster-Whisper).
        - Set `device="cuda"`.
        - Set the appropriate `language` (e.g., "en") if known.
        - Configure the tuned VAD parameters identified in Subtask 1.1 (e.g., `vad_threshold`).
        - Set the `microphone_index` parameter to use the custom input mechanism developed in Subtask 1.2.
    - Call `nova.configure_transcriptor(conditioning=...)`.

- [ ] **Subtask 1.4: Integrate `start_transcriptor` & `bind_context_source`**
    - Remove all usages of `stt_integration.py` (`stt_processor`).
    - During server startup (after configuring Nova):
        - Call `context_generator = nova.start_transcriptor()`. This should initiate the `VoiceAnalysis` process using the custom audio input.
        - Call `nova.bind_context_source(source=context_generator)`. This connects the `VoiceAnalysis` output (transcriptions based on VAD) to the `ContextManager`.

- [ ] **Subtask 1.5: Verify Context Updates**
    - Add logging within the server or periodically call `nova.get_context()` after a test call.
    - Check that `ContextDatapoint`s are being added to the context.
    - Verify the `source` of these datapoints indicates they come from the transcriptor (e.g., `ContextSource_Transcriptor` or similar, check `context_manager.py` or `transcriptor.py` for exact source type).
    - Confirm the `content` contains the expected transcriptions generated by Whisper Medium based on the VAD endpointing.

**Task 2: Refactor TTS Integration (Zonos Primary, ElevenLabs Option)**

- [ ] **Subtask 2.1: Integrate Local Zonos TTS**
    - Review `Nova2/app/inference_engines/inference_tts/inference_zonos.py` and `Nova2/examples/2. TTS/2.1 Running inference.ipynb`.
    - Define `TTSConditioning` for Zonos:
        - `model`: Specify the Zonos model (e.g., "Zyphra/Zonos-v0.1-transformer").
        - `voice`: Use default "Laura" or a custom cloned voice.
        - Set appropriate `expressivness`, `stability`, `language`, `speaking_rate`.
    - Investigate if `InferenceEngineZonos` supports streaming (`run_inference(..., stream=True)` returning an iterator). If not, this capability may need to be added, similar to the ElevenLabs modifications, or non-streaming TTS used as a fallback. Assume streaming needs verification/implementation for now.

- [ ] **Subtask 2.2: Implement Switchable TTS Engines**
    - Introduce a configuration setting (e.g., environment variable `TTS_ENGINE=zonos` or `TTS_ENGINE=elevenlabs`, read via Pydantic BaseSettings in FastAPI).
    - In the server's startup sequence:
        - Based on the config setting, conditionally instantiate either `InferenceEngineZonos()` or `InferenceEngineElevenlabs()`.
        - Create the corresponding `TTSConditioning` object.
        - Call `nova.configure_tts(...)` and `nova.apply_config_tts(...)` with the selected engine and conditioning.
    - Modify the `stream_telemarketer.py` wrapper or replace it. The `handle_next_action` function in `telemarketer_server.py` (or wherever TTS is triggered) should simply call `nova.run_tts(text, stream=True)`.

- [ ] **Subtask 2.3: Ensure Consistent Output Handling**
    - Verify that `nova.run_tts(..., stream=True)` returns a `StreamingAudioData` iterator for *both* the (potentially modified) Zonos engine and the existing ElevenLabs engine.
    - Confirm the loop in `handle_next_action` (or similar) correctly iterates through the `StreamingAudioData` object.
    - Ensure chunks from the iterator are passed through `audio_utils.convert_chunk` (or `convert_stream`) before being sent over the Twilio WebSocket. Check if `audio_utils` needs adjustment for Zonos output format (it currently assumes MP3 input, Zonos might output WAV or raw PCM).

**Task 3: Refine LLM Interaction Trigger**

- [ ] **Subtask 3.1: Implement Context-Based LLM Trigger**
    - **Strategy:** Use an event-driven or polling mechanism tied to `ContextManager` updates.
    - **Option A (Polling - Simpler):** In the main server logic or a dedicated background task, periodically (`asyncio.sleep`) call `nova.get_context()`. Compare the latest context state (e.g., number of datapoints or timestamp of last transcriptor input) with the previously seen state. If a new transcription datapoint is detected, trigger the LLM call sequence.
    - **Option B (Event-Driven - More Complex):** Modify `ContextManager.add_to_context` to emit an event (e.g., using `asyncio.Event` or a pub/sub library) when a relevant datapoint (from transcriptor) is added. A listener task awaits this event to trigger the LLM sequence.
    - Select and implement one approach. The trigger logic should likely reside within `telemarketer_server.py` or be called from the context update point.

- [ ] **Subtask 3.2: Integrate LLM Prompt/Script**
    - Create a separate configuration file (e.g., `prompts.yaml`) or Python module (`llm_prompts.py`) to store the detailed system prompt (persona, instructions, rules, goal) for the telemarketer LLM.
    - In the LLM trigger logic (from Subtask 3.1):
        1. Load the system prompt.
        2. Get current context: `context = nova.get_context()`.
        3. Convert to conversation: `conversation = context.to_conversation()`.
        4. Create system message: `system_message = Message(author="system", content=loaded_system_prompt)`.
        5. Prepend system message: `conversation.messages.insert(0, system_message)` (or similar method if `Conversation` provides one).
        6. Configure LLM (Groq Llama 3.3 70B via `LLMConditioning`) if not already done globally. Ensure `nova.configure_llm` and `nova.apply_config_llm` have been called.
        7. Load necessary tools (e.g., `end_call` tool): `tools = nova.load_tools(...)`.
        8. Call LLM: `llm_response = nova.run_llm(conversation=conversation, tools=tools)`.
        9. Handle response (send to TTS, execute tool calls: `nova.execute_tool_calls(llm_response)`).

**Task 4: Review & Enhance Twilio Integration**

- [ ] **Subtask 4.1: Review WebSocket Audio Reception**
    - In `/twilio/stream`, confirm the parsing of `media` messages from Twilio.
    - Ensure the extracted `payload` (audio bytes) is correctly decoded (likely base64) if necessary.
    - Verify these bytes are being reliably fed into the custom input mechanism for `VoiceAnalysis` (developed in Task 1.2).
    - Add error handling for WebSocket messages that are not of the expected format (`connected`, `start`, `media`, `stop`).

- [ ] **Subtask 4.2: Review WebSocket Audio Transmission**
    - In the TTS handling logic (e.g., within `handle_next_action`), review the loop that iterates through `StreamingAudioData`, converts chunks via `audio_utils`, and sends them back to Twilio via `websocket.send_bytes()`.
    - Ensure appropriate `try...except` blocks handle potential WebSocket errors during sending.
    - Consider adding `await asyncio.sleep(0.01)` within the sending loop if backpressure or CPU starvation becomes an issue, but test performance first.
    - Verify the `mark` messages for labeling segments are still relevant or needed.
    - Ensure `stop` messages from Twilio are handled gracefully to terminate processing for the call.

**Task 5: Prepare for Dashboard Integration**

- [ ] **Subtask 5.1: Identify Key Dashboard Data**
    - Call Metadata: `call_sid`, `start_time`, `end_time`, `duration`, `from_number`, `to_number`, `is_outbound`.
    - Status & State: Current `ConversationState`, final status (`completed`, `error`, `no-answer`).
    - Conversation Log: Full transcription (interleaved user/assistant turns), potentially including timestamps. Can be reconstructed from `CallStateMachine.history` or `ContextManager` data.
    - LLM Interaction: LLM responses, tools called, reasons for actions (e.g., `LLMAction` data).
    - Metrics: Sentiment scores (if available), keyword hits, intent detection history (already tracked in `CallStateMachine`).

- [ ] **Subtask 5.2: Propose API Endpoints & State Manager Additions**
    - **`CallStateManager` Additions:**
        - Method `get_recent_calls(limit: int = 50, offset: int = 0)`: Retrieve a list of calls from the `calls` table, ordered by time.
        - Method `get_call_details(call_sid: str)`: Retrieve full details for a single call (potentially joining history or reconstructing from persisted state).
    - **FastAPI Endpoints (`telemarketer_server.py`):**
        - `GET /api/calls`: Uses `CallStateManager.get_recent_calls` to return a list of basic call info. Add pagination parameters (`limit`, `offset`).
        - `GET /api/calls/{call_sid}`: Uses `CallStateManager.get_call_details` to return detailed info for one call, including history/transcript.
        - `GET /api/calls/active`: (Modify existing) Ensure it accurately reflects calls currently being processed based on `active_calls` or state in DB.
    - Ensure endpoints return data using Pydantic models for clear API contracts.
